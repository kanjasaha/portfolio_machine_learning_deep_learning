{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n# Kaggle https://www.kaggle.com/c/ieee-fraud-detection\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Activation\nfrom sklearn.model_selection import StratifiedKFold,cross_val_score\nfrom keras.utils.generic_utils import get_custom_objects\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_identity = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_identity.csv\")\ntrain_transaction = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")\ntrain_identity = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/sample_submission.csv\")\ntest_transaction = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\")\ntrain=train_transaction.merge(train_identity, left_on=\"TransactionID\",right_on=\"TransactionID\", how=\"left\",suffixes=('_tran','_iden'))\ntest=test_transaction.merge(test_identity, left_on=\"TransactionID\",right_on=\"TransactionID\", how=\"left\",suffixes=('_tran','_iden'))\nprint (\"{} dataset has {} rows(samples) with {} columns(features) each.\".format('train',*train.shape))  \nprint (\"{} dataset has {} rows(samples) with {} columns(features) each.\".format('test',*test.shape))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nscaler = MinMaxScaler()\n\nId = test['TransactionID']\ntrain = train.drop('TransactionID',axis =1)\ntest = test.drop('TransactionID',axis =1)\ny_train_d=train['isFraud']\nX_train_d=train.drop(['isFraud'],axis=1)   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_n=pd.DataFrame(scaler.fit_transform(X_train_d),columns=X_train_d.columns,index=X_train_d.index)\ntest_n=pd.DataFrame(scaler.fit_transform(test),columns=test.columns,index=test.index)\nX_train, X_validate, y_train, y_validate = train_test_split(train_n, y_train_d, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom sklearn.metrics import roc_auc_score\nfrom keras.layers import LeakyReLU,Activation\nfrom keras.callbacks import TensorBoard\nimport sys\n\nfrom keras import callbacks\nfrom keras import backend as K\n\ndef getBestModelfromTrials(trials):\n    valid_trial_list = [trial for trial in trials\n                            if STATUS_OK == trial['result']['status']]\n    losses = [ float(trial['result']['loss']) for trial in valid_trial_list]\n    index_having_minumum_loss = np.argmin(losses)\n    best_trial_obj = valid_trial_list[index_having_minumum_loss]\n    return best_trial_obj['result']['Trained_Model']\n\nclass MyLogger(callbacks.Callback):\n    def __init__(self, n):\n        self.n = n   # print loss & acc every n epochs\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.n == 0:\n            curr_loss =logs.get('loss')\n            curr_acc = logs.get('acc') * 100\n            print(\"epoch = %4d  loss = %0.6f  acc = %0.2f%%\" % (epoch, curr_loss, curr_acc))\n    \n    \n\n\ndef swish(x):\n    return (K.sigmoid(x) * x)\n\nmy_logger = MyLogger(n=1)\n#my_logger = TensorBoard(log_dir=\"logs/{}\".format(time()))\n#my_logger = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n#tensorboard --logdir output/Graph \n#class_weight = {0: 1., 1: 9.}\nspace = {'choice': hp.choice('num_layers',\n                    [ {'layers':'two', },\n                    {'layers':'three',\n                    'units3': hp.uniform('units3', 64,1024), \n                    'dropout3': hp.uniform('dropout3', .25,.75)}\n                    ]),\n\n            'units1': hp.uniform('units1', 64,1024),\n            'units2': hp.uniform('units2', 64,1024),\n\n            'dropout1': hp.uniform('dropout1', .25,.75),\n            'dropout2': hp.uniform('dropout2',  .25,.75),\n\n            'batch_size' : hp.uniform('batch_size', 28,128),\n\n            'nb_epochs' :  100,\n            'optimizer': hp.choice('optimizer',['adam']),\n            'activation': hp.choice('activation',['LeakyReLU'])\n        }\n\ndef f_nn(params):   \n    from keras.models import Sequential\n    from keras.layers.core import Dense, Dropout, Activation\n    from keras.optimizers import Adadelta, Adam, rmsprop\n\n    print ('Params testing: ', params)\n    #print(\"shape testing: {}, {}\".format(params['choice']['units3']))\n    model = Sequential()\n   \n    model.add(Dense(units=round(params['units1']), input_dim = X_train.shape[1], kernel_initializer = \"glorot_uniform\")) \n    if params['activation']== 'LeakyReLU':\n        model.add(LeakyReLU()) \n    else:\n        model.add(Activation(params['activation']))\n    model.add(Dropout(params['dropout1']))\n\n    model.add(Dense(units=round(params['units2']), kernel_initializer = \"glorot_uniform\")) \n    if params['activation']== 'LeakyReLU':\n        model.add(LeakyReLU()) \n    else:\n        model.add(Activation(params['activation']))\n    model.add(Dropout(params['dropout2']))\n\n    if params['choice']['layers']== 'three':\n        model.add(Dense(units=round(params['choice']['units3']), kernel_initializer = \"glorot_uniform\")) \n        if params['activation']== 'LeakyReLU':\n            model.add(LeakyReLU()) \n        else:\n            model.add(Activation(params['activation']))\n        model.add(Dropout(params['choice']['dropout3']))    \n\n    model.add(Dense(units=1, kernel_initializer='normal'))\n    model.add(Activation('sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n    \n    model.fit(X_train, y_train, epochs=round(params['nb_epochs']), batch_size=round(params['batch_size']),verbose = 0,callbacks=[my_logger]\n              #, class_weight=class_weight\n             )\n\n    pred_auc =model.predict_proba(X_validate, batch_size = 128, verbose = 0)\n    acc = roc_auc_score(y_validate, pred_auc)\n    print('AUC:', acc)\n    sys.stdout.flush() \n    return {'loss': -acc, 'status': STATUS_OK}\n\n\ndef optimize(objective,space,evals, cores, trials, optimizer=tpe.suggest, random_state=0):\n    \n    best = fmin(objective, space, algo=tpe.suggest, max_evals=evals, trials = trials)\n    trials = Trials()\n    best_param = fmin(f_nn, space, algo=tpe.suggest, max_evals=10, trials=trials)\n    best_model = getBestModelfromTrials(trials)\n    return best_param,best_model\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trials = Trials()\ncores = 32\nn= 1000\nstart = time.time()\nbest_param,best_model = optimize(f_nn,space,evals = n, cores = cores,optimizer=tpe.suggest,trials = trials)\nprint(\"------------------------------------\")\nprint(\"The best hyperparameters are: \", \"\\n\")\nprint(best_param)\nend = time.time()\nprint('Time elapsed to optimize {0} executions: {1}'.format(n,end - start))\n\nprint ('best_model: ')\nprint (best_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_class=best_model.predict(test_n)\nunscaled = scale_target.inverse_transform(pd.DataFrame(y_pred))\nunscaled_df=pd.DataFrame(unscaled)\n#unscaled_df.head()\n\nsubs_null = pd.DataFrame(index=testrows_with_null_value.index)\nsubs_null['id'] = testrows_with_null_value.index\nsubs_null['revenue'] = np.mean(unscaled)\n#subs_null\n## submission\nsubs = pd.DataFrame(index=test_n.index)\nsubs['id'] = test_n.index\nsubs['revenue'] = unscaled\nsubs_final = pd.concat([subs,subs_null], ignore_index=True)\nsubs_final.to_csv('submission.csv', index=False,float_format='%.0f')\nsubs_final.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}