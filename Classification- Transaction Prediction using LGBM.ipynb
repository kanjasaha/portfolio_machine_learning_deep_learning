{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"zMfol4A79myL","colab_type":"code","outputId":"e37322d4-72b7-4c05-d8c7-63b579bb2392","colab":{}},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n# kaggle : https://www.kaggle.com/c/santander-customer-transaction-prediction\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Activation\nfrom sklearn.model_selection import StratifiedKFold,cross_val_score\nfrom keras.utils.generic_utils import get_custom_objects\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"id":"z97IYp0y9myR","colab_type":"code","outputId":"588a40d3-2ee0-485c-fa2d-693622c31ae6","colab":{}},"cell_type":"code","source":"#Checking to see the expected submission format\nsample = pd.read_csv('../input/sample_submission.csv')\n\nprint(sample.head())\n\ntrain = pd.read_csv('../input/train.csv')\nprint (\"Train dataset has {} rows(samples) with {} columns(features) each.\".format(*train.shape))\n\n#test = pd.read_csv('test.csv', index_col='Id')\ntest = pd.read_csv('../input/test.csv')\n\nprint (\"Test dataset has {} rows(samples) with {} columns(features) each.\".format(*test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"n6WWZIMT9myV","colab_type":"code","colab":{}},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_hist(dataframe, plt,include_columns,bins=20):\n    display(('--------------------Histograms of each Numerical Feature--------------------'))\n   \n    for col in include_columns:\n        #if col in \n        plt.figure(figsize=(10,5))\n        display(col)\n        plt.hist(dataframe[col].dropna(),bins=bins)\n        #plt.gca().yaxis.set_major_formatter(ticker.IndexFormatter(1))\n        plt.xlabel(col)\n        plt.ylabel('Frequency')\n        plt.title('Histogram of '+ col,size=16, y=1.05)\n        plt.grid(True)\n        plt.show()\n\ndef get_categorical_variables(df,include_variables,exclude_variables,index_variable,label): \n    if include_variables is None: \n        include_variables=list(df.select_dtypes(include=[np.object,np.bool]).columns)\n    if index_variable in include_variables:\n          include_variables.remove(index_variable)\n\n    if label is not None:\n        if label in include_variables :\n            if exclude_variables is None:\n                 exclude_variables=label\n            else:\n                 exclude_variables=exclude_variables.append(label)\n\n    if exclude_variables is None:\n        include_variables_filtered=include_variables\n    else:\n        include_variables_filtered=[elem for elem in include_variables if elem not in exclude_variables] \n    return include_variables_filtered;\n\ndef get_numerical_variables(df,include_variables,exclude_variables,index_variable,label):\n    if include_variables is None:\n        include_variables=list(df.select_dtypes(exclude=[np.object,np.bool,np.datetime64]).columns)\n    if index_variable in include_variables:\n        include_variables.remove(index_variable)\n    if label is not None:\n        if label in include_variables :\n            if exclude_variables is None:\n                exclude_variables=label\n            else:\n                exclude_variables=exclude_variables.append(label)\n\n    if exclude_variables is None:\n          include_variables_filtered=include_variables\n    else:\n          include_variables_filtered=[elem for elem in include_variables if elem not in exclude_variables] \n    return include_variables_filtered;\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"id":"LtiWMIUp9myY","colab_type":"code","outputId":"f36bdce6-4b7d-4281-bb46-86dc79638c3e","colab":{}},"cell_type":"code","source":"index_variable='ID_code'\nlabel='target'\ninclude_numerical_variables=None # if None then we select all except the  exclude_numerical_variables\ninclude_categorical_variables=None # if None then we select all except the  exclude_categorical_variables\nexclude_numerical_variables=None\nexclude_categorical_variables=None\ncategorical_variables =get_categorical_variables(train,include_categorical_variables,exclude_categorical_variables,index_variable,label)\nnumerical_variables=get_numerical_variables(df=train,include_variables=None,exclude_variables=None,index_variable=index_variable,label=label)\n\ndisplay(('=====================Categorical Variables==================='))\ndisplay(categorical_variables)\n  \ndisplay(('=====================Numerical Variables====================='))\ndisplay(numerical_variables)\ndisplay(('=====================Dependent Variable====================='))\ndisplay(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"wwp-9NkF9myk","colab_type":"code","colab":{}},"cell_type":"code","source":"from sklearn import preprocessing\n\ndef normalize_data(data):\n    #normalize all the columns(features) so that all the values in the column lie between 0 and 1\n    #this way each features will get equal preference regardless of their actual range\n    n_data=pd.DataFrame()\n    n_data = pd.DataFrame(preprocessing.normalize(data),columns=data.columns)\n    return n_data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"2LPrSqEw9myx","colab_type":"code","colab":{}},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nid_code=train[index_variable]\ny=train[label]\nX=normalized_train=normalize_data(train[numerical_variables])\n#X_trast=normalized_train=normalize_data(test[numerical_variables])\nX_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_y_train = pd.concat([X_train,y_train],axis=1)\nX_y_train_1s = X_y_train[X_y_train['target'] == 1]\n\nX_y_train_0s = X_y_train[X_y_train['target'] == 0]\n#keep_0s = X_y_train_0s.sample(frac=X_y_train_1s.shape[0]/X_y_train_0s.shape[0]) for downsampling\nrep_1 =[X_y_train_1s for x in range(X_y_train_0s.shape[0]//X_y_train_1s.shape[0] )]\nkeep_1s = pd.concat(rep_1, axis=0)\nX_y_train = pd.concat([X_y_train_0s,keep_1s],axis=0)\n#X_y_train = pd.concat([keep_0s,X_y_train_1s],axis=0)\ny_train=X_y_train[label]\nX_train=X_y_train[numerical_variables]\nX_y_train.groupby('target').size()/train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#HYPEROPT FOR LGBM\nfrom hyperopt import STATUS_OK\nfrom timeit import default_timer as timer\nimport lightgbm as lgb\n\nMAX_EVALS = 50\nN_FOLDS = 10\nITERATION = 0\n# Define the search space\nspace = {\n    'class_weight': hp.choice('class_weight', [None, 'balanced']),\n    'boosting_type': hp.choice('boosting_type', [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)}, \n                                                 {'boosting_type': 'dart', 'subsample': hp.uniform('dart_subsample', 0.5, 1)},\n                                                 {'boosting_type': 'goss', 'subsample': 1.0}]),\n    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n    'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n    'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)\n}\n\n\ndef objective(params, n_folds = N_FOLDS):\n    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization\"\"\"\n    print ('Params testing: ', params)\n    # Keep track of evals\n    global ITERATION\n    \n    ITERATION += 1\n    \n    # Retrieve the subsample if present otherwise set to 1.0\n    subsample = params['boosting_type'].get('subsample', 1.0)\n    \n    # Extract the boosting type\n    params['boosting_type'] = params['boosting_type']['boosting_type']\n    params['subsample'] = subsample\n    \n    # Make sure parameters that need to be integers are integers\n    for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n        params[parameter_name] = int(params[parameter_name])\n    \n    start = timer()\n    \n    # Perform n_folds cross validation\n    train_set = lgb.Dataset(X_train, y_train)\n    cv_results = lgb.cv(params, train_set, num_boost_round = 10000, nfold = n_folds, \n                        early_stopping_rounds = 100, metrics = 'auc', seed = 50)\n    \n    run_time = timer() - start\n    \n    # Extract the best score\n    best_score = np.max(cv_results['auc-mean'])\n    \n    # Loss must be minimized\n    loss = 1 - best_score\n    \n    # Boosting rounds that returned the highest cv score\n    n_estimators = int(np.argmax(cv_results['auc-mean']) + 1)\n\n    # Write to the csv file ('a' means append)\n    of_connection = open(out_file, 'a')\n    writer = csv.writer(of_connection)\n    writer.writerow([loss, params, ITERATION, n_estimators, run_time])\n    \n    pred_auc =model.predict_proba(X_validate, batch_size = 128, verbose = 0)\n    acc = roc_auc_score(y_validate, pred_auc)\n    print('AUC:', acc)\n    sys.stdout.flush() \n   \n    # Dictionary with information for evaluation\n    return {'loss': loss, 'params': params, 'iteration': ITERATION,\n            'estimators': n_estimators, \n            'train_time': run_time, 'status': STATUS_OK}\n\nbayes_trials = Trials()\nbest = fmin(fn=objective,  space=space,  algo=tpe.suggest, max_evals=MAX_EVALS,trials = bayes_trials)\n\nprint(\"Hyperopt estimated optimum {}\".format(best))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nspace = {\n    'max_depth': hp.uniform('max_depth', 2, 4),\n    'n_estimators': hp.uniform('n_estimators', 20, 50),\n    'learning_rate': hp.uniform('learning_rate', 0.05, 1.0),\n    'max_features': hp.uniform('max_features', 2, 4),\n    'min_samples_leaf': hp.uniform('min_samples_leaf', 0.1, 0.5, 5),\n    'min_samples_split': hp.uniform('min_samples_split', 0.1, 1.0, 10),\n}\n\ndef objective(params):\n    params = {\n        'max_depth': int(params['max_depth']),\n        'n_estimators': '{:.3f}'.format(params['n_estimators']),\n        'learning_rate': int(params['learning_rate']),\n        'max_features': '{:.3f}'.format(params['max_features']),\n        'min_samples_leaf': int(params['min_samples_leaf']),\n        'min_samples_split': '{:.3f}'.format(params['min_samples_split']),\n    }\n    \n    clf = GradientBoostingClassifier(**params, random_state = 0)\n     \n    \n    score = cross_val_score(clf, X, Y, scoring=gini_scorer, cv=StratifiedKFold()).mean()\n    print(\"Gini {:.3f} params {}\".format(score, params))\n    return score\n\n\n\nbest = fmin(fn=objective,  space=space, algo=tpe.suggest, max_evals=MAX_EVALS)\n\nprint(\"Hyperopt estimated optimum {}\".format(best))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"DlU4kj4s9mzF","colab_type":"code","outputId":"db126136-2cc2-4687-acf1-bbdeb0d30b2f","colab":{}},"cell_type":"code","source":"\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n\nlearning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\nfor learning_rate in learning_rates:\n    gb = GradientBoostingClassifier(n_estimators=20, learning_rate = learning_rate, max_features=2, max_depth = 2, random_state = 0)\n    gb.fit(X_train, y_train)\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train, y_train)))\n    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_validate, y_validate)))\n    y_pred = gb.predict(X_validate)\n\n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_validate, y_pred))\n    print(\"Classification Report\")\n    print(classification_report(y_validate, y_pred))\n    \n    y_scores_gb = gb.decision_function(X_validate)\n    fpr_gb, tpr_gb, _ = roc_curve(y_validate, y_scores_gb)\n    roc_auc_gb = auc(fpr_gb, tpr_gb)\n\n    print(\"Area under ROC curve = {:0.2f}\".format(roc_auc_gb))\n    print()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"qYSdiDcf9mzJ","colab_type":"code","colab":{}},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import fbeta_score,accuracy_score,f1_score\n\ny_pred=model.predict_classes(X_validate, batch_size=32, verbose=1)\nlabels = [0,1]\ncm = confusion_matrix(y_validate, y_pred, labels)\ndisplay(cm)\nclass_error_fp=cm[0][1]/(cm[0][0]+cm[0][1])\nclass_error_fn=cm[1][0]/(cm[1][0]+cm[1][1])\n\nTest_set_accuracy_rate=accuracy_score(y_validate,y_pred)\n\nprint (\"Test set accuracy:{} Class_error_fp: {} Class_error_fn: {}\".format(Test_set_accuracy_rate,class_error_fp,class_error_fn))\n","execution_count":0,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(params):\n    params = {'n_estimators': int(params['n_estimators']), 'max_depth': int(params['max_depth'])}\n    clf = RandomForestClassifier(n_jobs=4, class_weight='balanced', **params)\n    score = cross_val_score(clf, X, Y, scoring=gini_scorer, cv=StratifiedKFold()).mean()\n    print(\"Gini {:.3f} params {}\".format(score, params))\n    return score\n\nspace = {\n    'n_estimators': hp.quniform('n_estimators', 25, 500, 25),\n    'max_depth': hp.quniform('max_depth', 1, 10, 1)\n}\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=10)\nGini -0.218 params {'n_estimators': 50, 'max_depth': 1}\nGini -0.243 params {'n_estimators': 175, 'max_depth': 4}\nGini -0.237 params {'n_estimat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(params):\n    params = {\n        'max_depth': int(params['max_depth']),\n        'gamma': \"{:.3f}\".format(params['gamma']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n    }\n    \n    clf = xgb.XGBClassifier(\n        n_estimators=250,\n        learning_rate=0.05,\n        n_jobs=4,\n        **params\n    )\n    \n    score = cross_val_score(clf, X, Y, scoring=gini_scorer, cv=StratifiedKFold()).mean()\n    print(\"Gini {:.3f} params {}\".format(score, params))\n    return score\n\nspace = {\n    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n    'gamma': hp.uniform('gamma', 0.0, 0.5),\n}\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=10)","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"KS - V7.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}