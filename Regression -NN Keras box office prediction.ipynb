{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n# Kaggle: https://www.kaggle.com/c/tmdb-box-office-prediction\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nimport collections as c\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking to see the expected submission format\nsample1 = pd.read_csv('../input/sample_submission.csv')\n\nprint(sample1.head())\n\ntrain1 = pd.read_csv('../input/train.csv')\nprint (\"Train dataset has {} rows(samples) with {} columns(features) each.\".format(*train1.shape))\nprint(train1.head(3))\n#test = pd.read_csv('test.csv', index_col='Id')\ntest1 = pd.read_csv('../input/test.csv')\n\nprint (\"Test dataset has {} rows(samples) with {} columns(features) each.\".format(*test1.shape))\nprint(test1.head(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def str_to_list(x):\n    return eval(x) if x and x != '#N/A' else []\n\nlist_cols = [\n    'genres',\n    'belongs_to_collection',\n    'production_companies',\n    'production_countries',\n    'spoken_languages',\n    'Keywords',\n    'cast',\n    'crew'\n]\n\nio_params = {\n    'index_col': 'id',\n    'converters': {col: str_to_list for col in list_cols}\n}\n\n# We can read the train and the test in one go using the concat function\ndata = pd.concat(\n    objs=(\n        pd.read_csv('../input/train.csv', **io_params).assign(is_train=True),\n        pd.read_csv('../input/test.csv', **io_params).assign(is_train=False)\n    ),\n    sort=False\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.head(1).transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['collection_name']=data['belongs_to_collection'].apply(lambda x: '' if len(x)==0 else x[0]['name'] )\ndata['has_collection'] = data['belongs_to_collection'].apply(lambda x: 0 if len(x)==0 else 1)\ndata.drop(['belongs_to_collection'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_genres = list(data['genres'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ndata['num_genres'] = data['genres'].apply(lambda x: len(x) if x != {} else 0)\ndata['all_genres'] = data['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_genres = [m[0] for m in c.Counter([i for j in list_of_genres for i in j])]\nfor g in top_genres:\n    data['genre_' + g] = data['all_genres'].apply(lambda x: 1 if g in x else 0)\ndata.drop(['genres'], axis=1, inplace=True)\ndata.drop(['all_genres'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_companies = list(data['production_companies'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ndata['num_companies'] = data['production_companies'].apply(lambda x: len(x) if x != {} else 0)\ndata['all_production_companies'] = data['production_companies'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nall_companies = [m[0] for m in c.Counter([i for j in list_of_companies for i in j])]\nfor g in all_companies:\n    data['production_company_' + g] = data['all_production_companies'].apply(lambda x: 1 if g in x else 0)\ndata.drop(['num_companies'], axis=1, inplace=True)\ndata.drop(['all_production_companies'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_countries = list(data['production_countries'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n                                                             \ndata['num_countries'] = data['production_countries'].apply(lambda x: len(x) if x != {} else 0)\ndata['all_countries'] = data['production_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\ntop_countries = [m[0] for m in c.Counter([i for j in list_of_countries for i in j])]\nfor g in top_countries:\n    data['production_country_' + g] = data['all_countries'].apply(lambda x: 1 if g in x else 0)\ndata.drop(['production_countries', 'all_countries'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_spoken_languages = list(data['spoken_languages'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\ndata['num_spoken_languages'] = data['spoken_languages'].apply(lambda x: len(x) if x != {} else 0)\ndata['all_spoken_languages'] = data['spoken_languages'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n\nall_spoken_languages=[]\nfor m in c.Counter([i for j in list_of_spoken_languages for i in j]):\n    if m=='':\n        m='Unknown'\n    all_spoken_languages.append(m[0])\n\nfor g in all_spoken_languages:\n    data['spoken_language_' + g] = data['all_spoken_languages'].apply(lambda x: 1 if g in x else 0)\ndata.drop(['spoken_languages', 'all_spoken_languages'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_Keywords = list(data['Keywords'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n                                                             \ndata['num_Keywords'] = data['Keywords'].apply(lambda x: len(x) if x != {} else 0)\ndata['all_Keywords'] = data['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\nall_Keywords = [m[0] for m in c.Counter([i for j in list_of_Keywords for i in j])]\nfor g in all_Keywords:\n    data['Keywords_' + g] = data['all_Keywords'].apply(lambda x: 1 if g in x else 0)\ndata.drop(['Keywords', 'all_Keywords'], axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To read an image from internet you must have the Internet Settings ON\n'''from  urllib.request import urlopen\nfrom PIL import Image\nurl=\"https://image.tmdb.org/t/p/w600_and_h900_bestv2/5VKVaTJJsyDeOzY6fLcyTo1RA9g.jpg\"\nimage = Image.open(urlopen(url))\nimage'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_cast_names = list(data['cast'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nlist_of_cast_genders = list(data['cast'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\nlist_of_cast_characters = list(data['cast'].apply(lambda x: [i['character'] for i in x] if x != {} else []).values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['num_cast'] = data['cast'].apply(lambda x: len(x) if x != {} else 0)\ntop_cast_names = [m[0] for m in c.Counter([i for j in list_of_cast_names for i in j]).most_common(15)]\nfor g in top_cast_names:\n    data['cast_name_' + g] = data['cast'].apply(lambda x: 1 if g in str(x) else 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['genders_0_cast'] = data['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ndata['genders_1_cast'] = data['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ndata['genders_2_cast'] = data['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ntop_cast_characters = [m[0] for m in c.Counter([i for j in list_of_cast_characters for i in j]).most_common(15)]\nfor g in top_cast_characters:\n    data['cast_character_' + g] = data['cast'].apply(lambda x: 1 if g in str(x) else 0)\ndata.drop(['cast'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_crew_names = list(data['crew'].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\nlist_of_crew_jobs = list(data['crew'].apply(lambda x: [i['job'] for i in x] if x != {} else []).values)\nlist_of_crew_genders = list(data['crew'].apply(lambda x: [i['gender'] for i in x] if x != {} else []).values)\nlist_of_crew_departments = list(data['crew'].apply(lambda x: [i['department'] for i in x] if x != {} else []).values)\n\ndata['num_crew'] = data['crew'].apply(lambda x: len(x) if x != {} else 0)\ntop_crew_names = [m[0] for m in c.Counter([i for j in list_of_crew_names for i in j]).most_common(15)]\nfor g in top_crew_names:\n    data['crew_name_' + g] = data['crew'].apply(lambda x: 1 if g in str(x) else 0)\ndata['genders_0_crew'] = data['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ndata['genders_1_crew'] = data['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ndata['genders_2_crew'] = data['crew'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\ntop_crew_jobs = [m[0] for m in c.Counter([i for j in list_of_crew_jobs for i in j]).most_common(15)]\nfor j in top_crew_jobs:\n    data['jobs_' + j] = data['crew'].apply(lambda x: sum([1 for i in x if i['job'] == j]))\ntop_crew_departments = [m[0] for m in c.Counter([i for j in list_of_crew_departments for i in j]).most_common(15)]\nfor j in top_crew_departments:\n    data['departments_' + j] = data['crew'].apply(lambda x: sum([1 for i in x if i['department'] == j])) \ndata.drop(['crew'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['has_homepage'] = 0\ndata.loc[data['homepage'].isnull() == False, 'has_homepage'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(columns=['homepage', 'imdb_id', 'original_language', 'original_title',\n        'overview', 'poster_path', 'production_companies', 'release_date',\n        'status', 'tagline', 'title', 'collection_name'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data ready\n#data.head()\ntrain=data[data['is_train']==1]\ntest=data[data['is_train']==0]\ntrain.drop(['is_train'],axis=1,inplace=True)\ntest.drop(['is_train'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dropna(inplace=True)\ntest.drop(['revenue'],axis=1,inplace=True)\ntestrows_with_null_value=test[test.isnull().any(axis=1)]\ntest.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n    \ntrain_n=pd.DataFrame(scaler.fit_transform(train),columns=train.columns,index=train.index)\ntest_n=pd.DataFrame(scaler.fit_transform(test),columns=test.columns,index=test.index)\ntrain_n.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_n.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eda = explore_data(train_n,None,None,None,None)\neda.explore_data_basic()\n#eda.show_numerical_variable_plots()\n#eda.show_categorical_variable_plots()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#id_code=train[index_variable]\ny_train_d=train_n['revenue']\nX_train_d=train_n.drop(['revenue'],axis=1)\n#X_trast=normalized_train=normalize_data(test[numerical_variables])\nX_train, X_validate, y_train, y_validate = train_test_split(X_train_d, y_train_d, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom keras.layers import LeakyReLU,Activation\nfrom keras.callbacks import TensorBoard\nfrom hyperopt.mongoexp import MongoTrials\nimport sys\n\nfrom keras import callbacks\nfrom keras import backend as K\n\nclass MyLogger(callbacks.Callback):\n    def __init__(self, n):\n        self.n = 10   # print loss & acc every n epochs\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.n == 0:\n            curr_loss =logs.get('loss')\n            curr_mse = logs.get('mean_squared_error') * 100\n            curr_mae = logs.get('mean_absolute_error') * 100\n            print(\"epoch = %4d  loss = %0.6f  mse = %0.2f  mae = %0.2f%%\" % (epoch, curr_loss, curr_mse,curr_mae))\n\ndef swish(x):\n    return (K.sigmoid(x) * x)\n  \n\nmy_logger = MyLogger(n=1)\nspace = {'choice': hp.choice('num_layers',\n                    [ {'layers':'two', }\n                    ]),\n\n            'units1': hp.uniform('units1', 64,164),\n            'units2': hp.uniform('units2', 64,164),\n\n            'dropout1': hp.uniform('dropout1', .25,.75),\n            'dropout2': hp.uniform('dropout2',  .25,.75),\n\n            'batch_size' : hp.uniform('batch_size', 28,128),\n\n            'nb_epochs' :  500,\n            'optimizer': hp.choice('optimizer',['adam']),\n            'activation': hp.choice('activation',['relu'])\n        }\n\ndef nn_objective(params):   \n    from keras.models import Sequential\n    from keras.layers.core import Dense, Dropout, Activation\n    from keras.optimizers import Adadelta, Adam, rmsprop\n\n    print ('Params testing: ', params)\n    model = Sequential()\n   \n    model.add(Dense(units=round(params['units1']),  activation=params['activation'], input_dim = X_train.shape[1], kernel_initializer = \"normal\")) \n   \n    model.add(Dense(units=round(params['units2']),  activation=params['activation'],kernel_initializer = \"normal\")) \n    \n    model.add(Dense(units=1, kernel_initializer='normal'))\n    model.compile(loss='mean_squared_error', optimizer=params['optimizer'], metrics=['mse','mae'])\n     \n    model.fit(X_train, y_train, epochs=round(params['nb_epochs']), validation_split=0.2, \n              batch_size=round(params['batch_size']),verbose = 0,callbacks=[my_logger])\n    #print(y_validate.head())\n    #y_pred = model.predict(X_validate)\n    #print(y_pred[:5])\n    score = model.evaluate(X_validate, y_validate, verbose=0, batch_size=15)\n    print('Validation data score:', score)\n    sys.stdout.flush() \n    return {'loss': score[0], 'status': STATUS_OK,'Trained_Model': model}\n    \n\n##trials = MongoTrials('mongo://localhost:1234/foo_db/jobs', exp_key='exp1')\ntrials = Trials()\n#best = fmin(nn_objective, space, algo=tpe.suggest, max_evals=10, trials=trials)\n\n#best_model = getBestModelfromTrials(trials)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import StratifiedKFold,cross_val_score\n\ndef base_lr_objective(params):\n    params = {\n        'max_depth': int(params['max_depth']),\n        'n_estimators': int(params['n_estimators']),\n        'learning_rate': float(params['learning_rate']),\n        'max_features': int(params['max_features']),\n        'min_samples_leaf': int(params['min_samples_leaf']),\n        'min_samples_split': int(params['min_samples_split']),\n         \n    }\n    \n    model = GradientBoostingRegressor(**params, random_state = 0)\n    model.fit(X_train, y_train)\n    print(params)\n    score=model.score(X_validate, y_validate)\n    #score = cross_val_score(clf, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n    #print(\"rmse {:.3f} params {}\".format(score, params))\n    print(score)\n    return  {'loss': score, 'status': STATUS_OK, 'Trained_Model': model}\n\nspace = {\n    'max_depth': hp.quniform('max_depth', 2, 4,1),\n    'n_estimators': hp.quniform('n_estimators', 20, 50,5),\n    'learning_rate': hp.uniform('learning_rate', 0.05, 1.0),\n    'max_features': hp.uniform('max_features', 2, 4),\n    'min_samples_leaf': hp.uniform('min_samples_leaf', 1, 5),\n    'min_samples_split': hp.quniform('min_samples_split', 2,4,1),\n  \n}\n\ntrials = Trials()\nbest = fmin(fn=base_lr_objective, space=space,algo=tpe.suggest,   max_evals=10,trials=trials)\n#best_model = getBestModelfromTrials(trials)\nprint(\"Hyperopt estimated optimum {}\".format(best))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getBestModelfromTrials(trials):\n    valid_trial_list = [trial for trial in trials\n                            if STATUS_OK == trial['result']['status']]\n    losses = [ float(trial['result']['loss']) for trial in valid_trial_list]\n    index_having_minumum_loss = np.argmin(losses)\n    best_trial_obj = valid_trial_list[index_having_minumum_loss]\n    return best_trial_obj['result']['Trained_Model']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = getBestModelfromTrials(trials)\nbest_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_n.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scale_target = MinMaxScaler()\ny_train_d=scale_target.fit_transform(train[['revenue']])\n\n\ny_pred=best_model.predict(test_n)\nunscaled = scale_target.inverse_transform(pd.DataFrame(y_pred))\nunscaled_df=pd.DataFrame(unscaled)\n#unscaled_df.head()\n\nsubs_null = pd.DataFrame(index=testrows_with_null_value.index)\nsubs_null['id'] = testrows_with_null_value.index\nsubs_null['revenue'] = np.mean(unscaled)\n#subs_null\n## submission\nsubs = pd.DataFrame(index=test_n.index)\nsubs['id'] = test_n.index\nsubs['revenue'] = unscaled\nsubs_final = pd.concat([subs,subs_null], ignore_index=True)\nsubs_final.to_csv('submission.csv', index=False,float_format='%.0f')\nsubs_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subs.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''import featuretools as ft\n\n# Make an entityset and add the entity\nes = ft.EntitySet(id = 'box_office')\nes.entity_from_dataframe(entity_id = 'data', dataframe = filtered, \n                         make_index = True, index = 'index')\n\n# Run deep feature synthesis with transformation primitives\nfeature_matrix, feature_defs = ft.dfs(entityset = es, target_entity = 'data',\n                                      trans_primitives = ['add_numeric', 'multiply_numeric'])\n\nfeature_matrix.head()'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n    \ndef transform_data(data,normalize_variables):\n    #normalize all the columns(features) so that all the values in the column lie between 0 and 1\n    #this way each features will get equal preference regardless of their actual range\n    n_data=pd.DataFrame(data=data)\n    n_data[normalize_variables] = scaler.fit_transform(data[normalize_variables])\n    #scaled = scaler.fit_transform(df)\n    #unscaled = scaler.inverse_transform(scaled)\n    return pd.get_dummies(n_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows_with_null_value=filtered[filtered.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import StratifiedKFold,cross_val_score\n\ndef gb_objective(params):\n    params = {\n        'max_depth': int(params['max_depth']),\n        'n_estimators': int(params['n_estimators']),\n        'learning_rate': float(params['learning_rate']),\n        'max_features': int(params['max_features']),\n        'min_samples_leaf': int(params['min_samples_leaf']),\n        'min_samples_split': int(params['min_samples_split']),\n         \n    }\n    \n    model = GradientBoostingRegressor(**params, random_state = 0)\n    model.fit(X_train, y_train)\n    print(params)\n    score=model.score(X_validate, y_validate)\n    #score = cross_val_score(clf, X_train, y_train, scoring='neg_mean_squared_error', cv=10)\n    #print(\"rmse {:.3f} params {}\".format(score, params))\n    print(score)\n    return  {'loss': score, 'status': STATUS_OK, 'Trained_Model': model}\n\nspace = {\n    'max_depth': hp.quniform('max_depth', 2, 4,1),\n    'n_estimators': hp.quniform('n_estimators', 20, 50,5),\n    'learning_rate': hp.uniform('learning_rate', 0.05, 1.0),\n    'max_features': hp.uniform('max_features', 2, 4),\n    'min_samples_leaf': hp.uniform('min_samples_leaf', 1, 5),\n    'min_samples_split': hp.quniform('min_samples_split', 2,4,1),\n  \n}\n\ntrials = Trials()\nbest = fmin(fn=objective, space=space,algo=tpe.suggest,   max_evals=10,trials=trials)\nbest_model = getBestModelfromTrials(trials)\nprint(\"Hyperopt estimated optimum {}\".format(best))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop('revenue',inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_n.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_n.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subs_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntestrows_with_null_value=rows_with_null_value[rows_with_null_value['is_train']==0]\ntestrows_with_null_value.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subs_null = pd.DataFrame(index=testrows_with_null_value.index)\nsubs_null['id'] = testrows_with_null_value['id']\nsubs_null['revenue'] = np.mean(unscaled)\nsubs_null","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(subs_final.head())\nsubs_final.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data manipulation libraries\nimport pandas as pd\nimport numpy as np\nimport itertools as iter\n\n#Visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import PercentFormatter\nfrom IPython.display import display,Markdown, HTML\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nfrom scipy.stats import chi2_contingency\n\nclass explore_data(object):\n    def __init__(self, dataframe,index_variable=None,label=None,include_variables=None,exclude_variables=None):\n        self.dataframe=dataframe\n        if index_variable is None:\n            self.index_variable=dataframe.columns[0]\n        else:\n            self.index_variable=index_variable\n        if label is None:\n            self.label=dataframe.columns[len(dataframe.columns)-1]\n        else:\n            self.label=index_variable\n        self.include_variables=include_variables\n        self.exclude_variables=exclude_variables\n        self.categorical_variables=self.get_categorical_variables()\n        self.numerical_variables=self.get_numerical_variables()\n  \n    def explore_data_basic(self):\n\n        # a quick look at the sample of the data set\n        display(('--------------------Sample Dataset--------------------'))\n        display(self.dataframe.head())\n\n        #Information on the data type\n        display(('--------------------Data Types of all columns--------------------'))\n        display(self.dataframe.columns.to_series().groupby(self.dataframe.dtypes).groups)\n\n        display(('--------------------% of 0 Values per column--------------------'))\n        print('{}'.format(self.dataframe.columns[self.dataframe.eq(0).any()].size))\n        for col in self.dataframe.columns[self.dataframe.eq(0).any()].tolist():\n        #print('{} has {} % null values'.format(col,(df[col].isnull().sum()*100.0)/df.shape[0]))\n          print('{} : {} % '.format(col,(self.dataframe[col].eq(0).sum()*100.0)/self.dataframe.shape[0]))\n\n        display(('--------------------% of Null Values per column--------------------'))\n        print('{}'.format(self.dataframe.columns[self.dataframe.isnull().any()].size))\n        for col in self.dataframe.columns[self.dataframe.isnull().any()].tolist():\n        #print('{} has {} % null values'.format(col,(df[col].isnull().sum()*100.0)/df.shape[0]))\n          print('{} : {} % '.format(col,(self.dataframe[col].isnull().sum()*100.0)/self.dataframe.shape[0]))\n\n        display(('--------------------% of NaN/Inf Values per column--------------------'))\n        #display(df.isin([np.nan, np.inf, -np.inf]).any().tolist())\n        display((self.dataframe.isin([np.nan, np.inf, -np.inf]).sum()*100.0)/self.dataframe.shape[0])\n\n\n        display(('--------------------Unique Values for each categorical Feature--------------------'))\n        for col in list(self.dataframe.select_dtypes(include=[np.object,np.bool]).columns):\n            display(self.dataframe[col].unique())\n\n        display(('--------------------5 Point Summary of Numeric Features--------------------'))\n        display(self.dataframe.describe())\n    \n    def show_numerical_variable_plots(self):\n        self.plot_hist()\n        self.box_plot()\n        self.scatter_plot_independent_label()\n        sns.set()\n        plt.figure(figsize=(10,10)) \n        sns.heatmap(df.corr(),annot=True,cmap=\"Blues\",fmt=\".2f\",linewidths=.05)\n        self.pearson_correlation_matrix()\n        self.check_for_outliers();\n        \n    def show_categorical_variable_plots(self):\n        self.percent_bar_chart()\n        self.chi_square_test()\n        self.bar_chart()\n        self.cat_plot_independent_label() \n        \n    def get_categorical_variables(self): \n        cat_variables=list(self.dataframe.select_dtypes(include=[np.object,np.bool]).columns)\n        if self.index_variable in cat_variables:\n            cat_variables.remove(self.index_variable)\n        \n        if self.label is not None:\n            if self.label in cat_variables :\n                cat_variables.remove(label)\n\n        if self.exclude_variables is None:\n            cat_variables_filtered=cat_variables\n        else:\n            cat_variables_filtered=[elem for elem in cat_variables if elem not in self.exclude_variables] \n        return cat_variables_filtered;\n    \n    def get_numerical_variables(self): \n        numerical_variables=list(self.dataframe.select_dtypes(include=[np.object,np.bool,np.datetime64]).columns)\n        if self.index_variable in numerical_variables:\n            numerical_variables.remove(self.index_variable)\n        \n        if self.label is not None:\n            if self.label in numerical_variables :\n                numerical_variables.remove(label)\n\n        if self.exclude_variables is None:\n            numerical_variables_filtered=numerical_variables\n        else:\n            numerical_variables_filtered=[elem for elem in cat_variables if elem not in self.exclude_variables] \n        return numerical_variables_filtered;\n    \n        \n    def plot_hist(self):\n        display(Markdown('**--------------------Histograms of each Feature--------------------**'))\n       \n        for col in self.numerical_variables:\n            plt.figure(figsize=(10,5))\n            #plt.hist(dataframe[col], bins=10)\n           # print(col)\n            s=self.dataframe[col].dropna()\n            #w = 100*(np.zeros_like(s) + 1. / len(s))\n            plt.hist(s)\n            plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n            plt.xlabel(col)\n            plt.ylabel('Frequency')\n            plt.title('Histogram of '+ col)\n            plt.grid(True)\n            plt.show()\n        \n    def bar_chart(self):\n        display(('--------------------bar chart of categorial variables--------------------'))\n   \n        for x in self.categorical_variables:\n           # if x != y:\n            sns.catplot(x=x, kind=\"count\", palette=\"deep\", data=self.dataframe,height=5, aspect=2);\n            plt.xlabel(x)\n            plt.xticks(rotation='vertical')\n            plt.ylabel('Frequency')\n            plt.title('Bar chart of '+ x ,size=16, y=1.05)\n            plt.grid(True)\n            plt.show()\n            display('==============================================================================================================')\n\n    def plot_density(self):\n        display(Markdown('**--------------------Density Plot of each Feature--------------------**'))\n   \n        for col in self.numerical_variables:\n            plt.figure(figsize=(10,5))\n            plt.hist(self.dataframe[col], bins=10)\n            plt.xlabel(col)\n            plt.ylabel('Frequency')\n            sns.distplot(self.dataframe[col], hist=True, kde=True, \n                 bins=int(180/5), color = 'darkblue', \n                 hist_kws={'edgecolor':'black'},\n                 kde_kws={'linewidth': 4})\n            plt.grid(True)\n            plt.show()\n            \n    def pair_plot(self):\n        display(Markdown('**--------------------Pairplot of all Features with continous numeric value--------------------**'))\n        for x,y in iter.combinations(include_columns,2):\n            #if x != y:\n                plt.figure(figsize=(10,5))\n                plt.scatter(dataframe[x],dataframe[y])\n                plt.xlabel(x)\n                plt.ylabel(y)\n                plt.title('Pairplot of '+ x + ' and ' + y)\n                plt.grid(True)\n                plt.show()\n\n    def bar_chart_xyz(self):\n        display(Markdown('**--------------------bar plot of each column segmented by another column--------------------**'))\n\n        for x,y,z in iter.combinations(include_columns,3):\n           # if x != y:\n            sns.catplot(x=x, hue=y,col=z, kind=\"count\", palette=\"deep\", data=dataframe,height=20, aspect=2);\n            plt.xlabel(x)\n            plt.ylabel(y)\n            plt.title('Bar plot of '+ x + ' and ' + y)\n            plt.grid(True)\n            plt.show()\n\n    def retention_funnel(dataframe,plt,include_columns,funnel_level,aggregate_column):\n        display(Markdown('**--------------------retention funnel of each column by billing cycles--------------------**'))\n\n        for x in include_columns:\n            if x !=funnel_level and x!=aggregate_column:\n                df_group=dataframe.groupby([funnel_level,x])[aggregate_column].agg('count').unstack()\n                pct=round((df_group.pct_change()+1)*100,0)\n                #display(pct)\n                pct.plot(figsize=(10,5))\n                plt.xlabel(x)     \n\n    def bar_chart_2_variables(self):\n        display(('--------------------bar plot of each categorical variables segmented by another categorical variables--------------------'))\n\n        for x,y in iter.combinations(include_columns,2):\n           # if x != y:\n            sns.catplot(x=x, hue=y, kind=\"count\", palette=\"deep\", data=dataframe,height=5, aspect=2);\n            plt.xticks(rotation='vertical')\n            plt.xlabel(x)\n            plt.ylabel(y)\n            plt.title('Bar plot of '+ x + ' and ' + y,size=16, y=1.05)\n            plt.grid(True)\n            plt.show()\n\n    def bar_chart_independent_label(self):\n        display(('--------------------bar plot of each categorical variables segmented by another categorical variables--------------------'))\n        y=label;\n        for x in include_columns:\n           # if x != y:\n            sns.catplot(x=x, hue=y, kind=\"count\", palette=\"deep\", data=dataframe,height=5, aspect=2);\n            plt.xticks(rotation='vertical')\n            plt.xlabel(x)\n            plt.ylabel(y)\n            plt.title('Bar plot of '+ x + ' and ' + y,size=16, y=1.05)\n            plt.grid(True)\n            plt.show()\n\n    def bar_chart(self):\n        display(Markdown('**--------------------bar plot of each column segmented by another column--------------------**'))\n\n        for x,y in iter.combinations(include_columns,2):\n           # if x != y:\n            sns.catplot(x=x, hue=y, kind=\"count\", palette=\"deep\", data=dataframe,height=5, aspect=2);\n            plt.xlabel(x)\n            plt.ylabel(y)\n            plt.title('Bar plot of '+ x + ' and ' + y)\n            plt.grid(True)\n            plt.show()\n\n    def percent_bar_chart(self):\n        display(Markdown('**--------------------bar plot of each column segmented by another column--------------------**'))\n\n        for x,y in iter.combinations(include_columns,2):\n           # if x != y:\n            freq_df = dataframe.groupby([x])[y].value_counts(normalize=True).unstack()\n            pct_df = freq_df.divide(freq_df.sum(axis=1), axis=0)\n            pct_df.plot(kind=\"bar\",figsize=(10,5))\n            plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n            plt.legend(loc=\"upper right\", bbox_to_anchor=(1.2,1.0))\n            plt.xlabel(x)\n            plt.ylabel(y)\n            plt.title('Bar plot of '+ x + ' and ' + y)\n            plt.grid(True)\n            plt.show()\n\n    def cat_plot_independent_label(self):\n        display(('--------------------bar plot of each categorical variables segmented by another categorical variables--------------------'))\n        y=label;\n        for x in include_columns:\n           # if x != y:\n            #sns.catplot(x=x, hue=y, kind=\"count\", palette=\"deep\", data=dataframe,height=5, aspect=2);\n            sns.catplot(x=x, y=y, data=df,height=5, aspect=2);\n            plt.xticks(rotation='vertical')\n            plt.xlabel(x)\n            plt.ylabel(y)\n            plt.title('Bar plot of '+ x + ' and ' + y,size=16, y=1.05)\n            plt.grid(True)\n            plt.show()\n\n\n    def scatter_plot_independent_label(self):\n        display(('--------------------scatter plot numerical variables by label--------------------'))\n        x=label;\n        for y in include_columns:\n           # if x != y:\n            plt.figure(figsize=(10,5))\n            sns.scatterplot(x=x, y=y, palette=\"deep\", data=dataframe);\n            plt.xticks(rotation='vertical')\n            plt.xlabel(x)\n            plt.ylabel(y)\n            plt.title('Scatter plot of '+ x + ' and ' + y,size=16, y=1.05)\n            plt.grid(True)\n            plt.show()\n\n    def percent_bar_chart(self):\n        display(('--------------------bar plot of each column segmented by another column--------------------'))\n\n        for x,y in iter.combinations(include_columns,2):\n           # if x != y:\n            freq_df = dataframe.groupby([x])[y].value_counts(normalize=True).unstack()\n            pct_df = freq_df.divide(freq_df.sum(axis=1), axis=0)\n            pct_df.plot(kind=\"bar\",figsize=(10,5))\n            plt.gca().yaxis.set_major_formatter(IndexFormatter(1))\n            plt.legend(loc=\"upper right\", bbox_to_anchor=(1.2,1.0))\n            plt.xlabel(x)\n            plt.ylabel(y)\n            plt.title('Bar plot of '+ x + ' and ' + y,size=16, y=1.05)\n            plt.grid(True)\n            plt.show()       \n\n    def percentage_barh_stacked_chart(self):\n        display(Markdown('**--------------------percentage chart for categorical variables--------------------**'))\n\n        for x,y in iter.combinations(include_columns,2):\n            #if x != y:\n                cont_table=pd.crosstab(dataframe[x],dataframe[y], normalize='index')\n                #print(cont_table)\n                cont_table.plot(kind='barh', stacked=True,figsize=(10,5))\n                plt.legend(loc=\"upper right\", bbox_to_anchor=(1.2,1.0))\n                plt.xlabel(x)\n                plt.ylabel(y)\n                plt.title('Bar plot of '+ x + ' and ' + y)\n                plt.grid(True)\n                plt.show()\n\n    def line_plot_multiple(self):\n        display(('--------------------time series plot of each variable --------------------'))\n\n        for x in categorical_variables:\n                plt.figure(figsize=(15,10))\n                df_d=dataframe.groupby([timeseries,x], as_index=False)[value].agg({'group_size':'sum'})\n                sns.lineplot(data=df_d, x=timeseries, hue=x,  y=\"group_size\")\n                plt.title('Time series plot of '+ x + ' and ' + timeseries,size=16, y=1.05)\n                plt.ylabel(x)\n                plt.legend(loc=\"upper right\", bbox_to_anchor=(1.2,1.0))\n                plt.xlabel(timeseries)\n                plt.grid(True)\n                plt.show()\n\n\n    def line_plot(self):\n        display(Markdown('**--------------------box plot of each column segmented by another column--------------------**'))\n\n        for x in categorical_variables:\n                plt.figure(figsize=(15,10))\n                df_d=dataframe.groupby([timeseries,x], as_index=False)[value].agg({'group_size':'sum'})\n                sns.lineplot(data=df_d, x=timeseries, hue=x,  y=\"group_size\") \n                plt.legend(loc=\"upper right\", bbox_to_anchor=(1.2,1.0))\n                plt.ylabel(x)\n                plt.xlabel(timeseries)\n                plt.title('Time series plot of '+ x + ' and ' + timeseries)\n                plt.grid(True)\n                plt.show()\n\n\n\n    def box_plot(self):\n        display(Markdown('**--------------------box plot of each column segmented by another column--------------------**'))\n\n        for x in categorical_variables:\n                plt.figure(figsize=(15,10))\n                sns.boxplot(x=x, y=numerical_variable,hue=segment, linewidth=2.5,data=dataframe)\n                plt.legend(loc=\"upper right\", bbox_to_anchor=(1.2,1.0))\n                plt.xlabel(x)\n                plt.ylabel(numerical_variable)\n                plt.title('Box plot of '+ x + ' and ' + numerical_variable)\n                plt.grid(True)\n                plt.show()\n\n    def box_plot_categorical_label(self):\n        display(('--------------------box plot of dependent varaible for categorical variable--------------------'))\n\n        for y in categorical_variables:\n                #sns.catplot(y=label, x=y, linewidth=2.5, kind=\"box\",data=dataframe,height=5, aspect=2)\n                plt.legend(loc=\"upper right\", bbox_to_anchor=(1.2,1.0))\n                plt.xticks(rotation='vertical')\n                plt.xlabel(label)\n                plt.ylabel(y)\n                sns.set_style(\"whitegrid\")\n                ax = sns.boxplot(x=y, y=label, data=dataframe)\n\n                plt.title('Box plot of '+ y,size=16, y=1.05 )\n                medians = dataframe.groupby([y])[label].median().values\n                median_labels = [str(np.round(s, 2)) for s in medians]\n\n                pos = range(len(medians))\n                for tick,label in zip(pos,ax.get_xticklabels()):\n                    ax.text(pos[tick], medians[tick] + 0.5, median_labels[tick], \n                    horizontalalignment='center', size='x-small', color='w', weight='semibold')\n                plt.grid(True)\n                plt.show()\n\n    # Pearson Correlation gives the correlation and p-value\n\n    def pearson_correlation_matrix(self):\n        display(('--------------------pearson correlation--------------------'))\n        corr_df = pd.DataFrame(columns=['feature', 'correlation', 'p_value'])\n        rows_list = []\n        y=label\n        for x in include_columns:\n         # if x != y:\n          display('Pearson Correlation between ' + x + ' and ' + y);\n          display(pearsonr(df[x],df[y]));\n          display('-----------------------------------------------');\n          correlation,p_value=pearsonr(df[x],df[y])\n          dict1 = {}\n          dict1 = {'column':x,'correlation':correlation,'p_value':p_value}\n          rows_list.append(dict1)\n\n        corr_df = pd.DataFrame(rows_list);  \n        display(corr_df);\n\n    # Correlation Matrix Heatmap\n    def correlation_matrix_continuous(self):\n        display(Markdown('**--------------------Correlation matrix continous numeric features--------------------**'))\n\n        f, ax = plt.subplots(figsize=(10, 6))\n        hm = sns.heatmap(round(df.corr(),2), annot=True, ax=ax, cmap=\"coolwarm\",fmt='.2f',\n                     linewidths=.05)\n        f.subplots_adjust(top=0.93)\n        t= f.suptitle('Correlation Heatmap', fontsize=14)   \n\n\n    def chi_square_test(self):\n        display(Markdown('**--------------------percentage chart for categorical variables--------------------**'))\n        #final_columns=[col for col in include_columns if col not in exclude_columns]\n\n        for x,y in iter.combinations(include_columns,2):\n            #if x != y:\n                crosstable=pd.crosstab(dataframe[x],dataframe[y], normalize='index')\n                cont_table=pd.crosstab(dataframe[x],dataframe[y])\n                #print(cont_table)\n                crosstable.plot(kind='barh', stacked=True,figsize=(10,5))\n                plt.legend(loc=\"upper right\", bbox_to_anchor=(1.2,1.0))\n                #plt.xlabel(x)\n                plt.ylabel(x)\n                plt.title('Bar plot of '+ x + ' and ' + y)\n                plt.grid(True)\n                plt.show()\n                chi2,p,df=chi2_contingency(cont_table)[0:3]\n                print('**The Null and Alternate Hypotheses**')\n\n                print('H0:There is no statistically significant relationship between the two selected variables')\n                print('Ha:There is a statistically significant relationship between the two selected variables')\n\n\n\n                if p < .05:\n                    print(\"We can reject the Null Hypothesis and say that \" + x + \" and \" + y + \" have some relationship\")\n                else:\n                    print(\"We cannot reject the Null hypothesis and say that \" + x + \" and \" + y + \" are truly independent\")\n                print()\n                print (\"X2: {}, p-value: {}, Degrees of Freedom: {}\".format(chi2,p,df))\n\n    def check_for_outliers(self):\n\n    # For each feature find the data points with extreme high or low values\n        log_data=data[include_variables]\n        total_count=len(log_data)\n        x=[]\n        for feature in log_data.keys():\n\n            # TODO: Calculate Q1 (25th percentile of the data) for the given feature\n            Q1 = np.percentile(log_data[feature],25)\n\n            # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n            Q3 = np.percentile(log_data[feature],75)\n\n            # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n            step = 1.5*(Q3-Q1)\n            y= log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))]\n            y1=y.index.values\n            x.append(y1)\n\n            # Display the outliers\n            feature_outlier_count=y.shape[0]\n            feature_percent_outliers=(float(feature_outlier_count)*100)/(float(total_count))\n            print (\"'{} ({:2.2f}%) Data points out of {} rows considered outliers for the feature '{}':\".format(feature_outlier_count,feature_percent_outliers,total_count,feature))\n\n\n            # OPTIONAL: Select the indices for data points you wish to remove\n            # Here I go through the lists and extract the index value that is repeated in more than one list.\n            seen = set()\n            repeated = set()\n\n        for l in x:\n            for i in set(l):\n                if i in seen:\n                  repeated.add(i)\n                else:\n                  seen.add(i)\n\n        #display dataset outlier\n\n        if log_data.shape[1] > 1:\n          outliers =list(repeated)\n          outlier_count=len(outliers)\n          percent_outliers=(float(outlier_count)*100)/(float(total_count))\n          print(\"{} ({:2.2f}%) data points considered outliers from the dataset of {}.\".format(outlier_count,percent_outliers,total_count))   \n\n    def pca_results(self,n_components):\n\n      pca = PCA(copy=True, iterated_power='auto', n_components=n_components, random_state=None,\n        svd_solver='auto', tol=0.0, whiten=False)\n      df_pca=df[include_variables]\n      pca.fit(df_pca)\n\n\n      dimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n\n      # PCA components\n      components = pd.DataFrame(np.round(pca.components_, 4), columns = list(df_pca.keys()))\n      components.index = dimensions\n\n      # PCA explained variance\n      ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n      variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n      variance_ratios.index = dimensions\n\n      # Create a bar plot visualization\n      fig, ax = plt.subplots(figsize = (14,8))\n\n      # Plot the feature weights as a function of the components\n      components.plot(ax = ax, kind = 'bar');\n      ax.set_ylabel(\"Feature Weights\")\n      ax.set_xticklabels(dimensions, rotation=0)\n\n\n          # Display the explained variance ratios\n      for i, ev in enumerate(pca.explained_variance_ratio_):\n          ax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n          %.4f\"%(ev))\n\n      # Return a concatenated DataFrame\n      pca_results=pd.concat([variance_ratios, components], axis = 1)\n\n      print(pca_results['Explained Variance'].cumsum())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}