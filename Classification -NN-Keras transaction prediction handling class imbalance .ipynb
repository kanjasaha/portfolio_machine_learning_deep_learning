{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"zMfol4A79myL","colab_type":"code","outputId":"e37322d4-72b7-4c05-d8c7-63b579bb2392","colab":{}},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n# kaggle https://www.kaggle.com/c/santander-customer-transaction-prediction\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Activation\nfrom sklearn.model_selection import StratifiedKFold,cross_val_score\nfrom keras.utils.generic_utils import get_custom_objects\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"id":"z97IYp0y9myR","colab_type":"code","outputId":"588a40d3-2ee0-485c-fa2d-693622c31ae6","colab":{}},"cell_type":"code","source":"#Checking to see the expected submission format\nsample = pd.read_csv('../input/sample_submission.csv')\n\nprint(sample.head())\n\ntrain = pd.read_csv('../input/train.csv')\nprint (\"Train dataset has {} rows(samples) with {} columns(features) each.\".format(*train.shape))\n\n#test = pd.read_csv('test.csv', index_col='Id')\ntest = pd.read_csv('../input/test.csv')\n\nprint (\"Test dataset has {} rows(samples) with {} columns(features) each.\".format(*test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"n6WWZIMT9myV","colab_type":"code","colab":{}},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef get_categorical_variables(df,include_variables,exclude_variables,index_variable,label): \n    if include_variables is None: \n        include_variables=list(df.select_dtypes(include=[np.object,np.bool]).columns)\n    if index_variable in include_variables:\n          include_variables.remove(index_variable)\n\n    if label is not None:\n        if label in include_variables :\n            if exclude_variables is None:\n                 exclude_variables=label\n            else:\n                 exclude_variables=exclude_variables.append(label)\n\n    if exclude_variables is None:\n        include_variables_filtered=include_variables\n    else:\n        include_variables_filtered=[elem for elem in include_variables if elem not in exclude_variables] \n    return include_variables_filtered;\n\ndef get_numerical_variables(df,include_variables,exclude_variables,index_variable,label):\n    if include_variables is None:\n        include_variables=list(df.select_dtypes(exclude=[np.object,np.bool,np.datetime64]).columns)\n    if index_variable in include_variables:\n        include_variables.remove(index_variable)\n    if label is not None:\n        if label in include_variables :\n            if exclude_variables is None:\n                exclude_variables=label\n            else:\n                exclude_variables=exclude_variables.append(label)\n\n    if exclude_variables is None:\n          include_variables_filtered=include_variables\n    else:\n          include_variables_filtered=[elem for elem in include_variables if elem not in exclude_variables] \n    return include_variables_filtered;\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"id":"LtiWMIUp9myY","colab_type":"code","outputId":"f36bdce6-4b7d-4281-bb46-86dc79638c3e","colab":{}},"cell_type":"code","source":"index_variable='ID_code'\nlabel='target'\ninclude_numerical_variables=None # if None then we select all except the  exclude_numerical_variables\ninclude_categorical_variables=None # if None then we select all except the  exclude_categorical_variables\nexclude_numerical_variables=None\nexclude_categorical_variables=None\ncategorical_variables =get_categorical_variables(train,include_categorical_variables,exclude_categorical_variables,index_variable,label)\nnumerical_variables=get_numerical_variables(df=train,include_variables=None,exclude_variables=None,index_variable=index_variable,label=label)\n\ndisplay(('=====================Categorical Variables==================='))\ndisplay(categorical_variables)\n  \ndisplay(('=====================Numerical Variables====================='))\ndisplay(numerical_variables)\ndisplay(('=====================Dependent Variable====================='))\ndisplay(label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"wwp-9NkF9myk","colab_type":"code","colab":{}},"cell_type":"code","source":"from sklearn import preprocessing\n\ndef normalize_data(data):\n    #normalize all the columns(features) so that all the values in the column lie between 0 and 1\n    #this way each features will get equal preference regardless of their actual range\n    n_data=pd.DataFrame()\n    n_data = pd.DataFrame(preprocessing.normalize(data),columns=data.columns)\n    return n_data\n\n#def remove_outliers_by_value(data, column,value):\n#    good_data = data.loc[~data.[column].isin(value)]\n#    return good_data\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"2LPrSqEw9myx","colab_type":"code","colab":{}},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nid_code=train[index_variable]\ny=train[label]\nX=normalized_train=normalize_data(train[numerical_variables])\n#X_trast=normalized_train=normalize_data(test[numerical_variables])\nX_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"vWqXzU7C9my0","colab_type":"code","colab":{}},"cell_type":"code","source":"train.groupby('target').size()/train.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"DlU4kj4s9mzF","colab_type":"code","outputId":"db126136-2cc2-4687-acf1-bbdeb0d30b2f","colab":{}},"cell_type":"code","source":"'''from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom sklearn.metrics import roc_auc_score\nfrom keras.layers import LeakyReLU,Activation\nfrom keras.callbacks import TensorBoard\nimport sys\n\nfrom keras import callbacks\nfrom keras import backend as K\n\nclass MyLogger(callbacks.Callback):\n    def __init__(self, n):\n        self.n = n   # print loss & acc every n epochs\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.n == 0:\n            curr_loss =logs.get('loss')\n            curr_acc = logs.get('acc') * 100\n            print(\"epoch = %4d  loss = %0.6f  acc = %0.2f%%\" % (epoch, curr_loss, curr_acc))\n    \n    \n\n\ndef swish(x):\n    return (K.sigmoid(x) * x)\n\nmy_logger = MyLogger(n=1)\n#my_logger = TensorBoard(log_dir=\"logs/{}\".format(time()))\n#my_logger = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n#tensorboard --logdir output/Graph \nclass_weight = {0: 1., 1: 9.}\nspace = {'choice': hp.choice('num_layers',\n                    [ {'layers':'two', },\n                    {'layers':'three',\n                    'units3': hp.uniform('units3', 64,1024), \n                    'dropout3': hp.uniform('dropout3', .25,.75)}\n                    ]),\n\n            'units1': hp.uniform('units1', 64,1024),\n            'units2': hp.uniform('units2', 64,1024),\n\n            'dropout1': hp.uniform('dropout1', .25,.75),\n            'dropout2': hp.uniform('dropout2',  .25,.75),\n\n            'batch_size' : hp.uniform('batch_size', 28,128),\n\n            'nb_epochs' :  100,\n            'optimizer': hp.choice('optimizer',['adam']),\n            'activation': hp.choice('activation',['LeakyReLU'])\n        }\n\ndef f_nn(params):   \n    from keras.models import Sequential\n    from keras.layers.core import Dense, Dropout, Activation\n    from keras.optimizers import Adadelta, Adam, rmsprop\n\n    print ('Params testing: ', params)\n    #print(\"shape testing: {}, {}\".format(params['choice']['units3']))\n    model = Sequential()\n   \n    model.add(Dense(units=round(params['units1']), input_dim = X_train.shape[1], kernel_initializer = \"glorot_uniform\")) \n    if params['activation']== 'LeakyReLU':\n        model.add(LeakyReLU()) \n    else:\n        model.add(Activation(params['activation']))\n    model.add(Dropout(params['dropout1']))\n\n    model.add(Dense(units=round(params['units2']), kernel_initializer = \"glorot_uniform\")) \n    if params['activation']== 'LeakyReLU':\n        model.add(LeakyReLU()) \n    else:\n        model.add(Activation(params['activation']))\n    model.add(Dropout(params['dropout2']))\n\n    if params['choice']['layers']== 'three':\n        model.add(Dense(units=round(params['choice']['units3']), kernel_initializer = \"glorot_uniform\")) \n        if params['activation']== 'LeakyReLU':\n            model.add(LeakyReLU()) \n        else:\n            model.add(Activation(params['activation']))\n        model.add(Dropout(params['choice']['dropout3']))    \n\n    model.add(Dense(units=1, kernel_initializer='normal'))\n    model.add(Activation('sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n    \n    model.fit(X_train, y_train, epochs=round(params['nb_epochs']), batch_size=round(params['batch_size']),verbose = 0,callbacks=[my_logger], class_weight=class_weight)\n\n    pred_auc =model.predict_proba(X_validate, batch_size = 128, verbose = 0)\n    acc = roc_auc_score(y_validate, pred_auc)\n    print('AUC:', acc)\n    sys.stdout.flush() \n    return {'loss': -acc, 'status': STATUS_OK}\n\n\ntrials = Trials()\nbest = fmin(f_nn, space, algo=tpe.suggest, max_evals=50, trials=trials)\nprint ('best: ')\nprint (best)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom sklearn.metrics import roc_auc_score\nfrom keras.layers import LeakyReLU,Activation\nfrom keras.callbacks import TensorBoard\nimport sys\n\nfrom keras import callbacks\nfrom keras import backend as K\n\nclass MyLogger(callbacks.Callback):\n    def __init__(self, n):\n        self.n = n   # print loss & acc every n epochs\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.n == 0:\n            curr_loss =logs.get('loss')\n            curr_acc = logs.get('acc') * 100\n            print(\"epoch = %4d  loss = %0.6f  acc = %0.2f%%\" % (epoch, curr_loss, curr_acc))\n    \n    \n#Params testing:                                                                  \nparams={'activation': 'LeakyReLU', 'batch_size': 121.49461530883102, 'choice': {'dropout3': 0.28556934532942824, 'layers': 'three', 'units3': 461.4279574939098}, \n 'dropout1': 0.5488421886416901, 'dropout2': 0.7395325011437675, 'nb_epochs': 100, 'optimizer': 'adam', 'units1': 686.4798766184405, 'units2': 450.82106283335355}\n\nmy_logger = MyLogger(n=1)\n#my_logger = TensorBoard(log_dir=\"logs/{}\".format(time()))\n#my_logger = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n#tensorboard --logdir output/Graph \nclass_weight = {0: 1., 1: 9.}\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.optimizers import Adadelta, Adam, rmsprop\n\nprint ('Params testing: ', params)\n#print(\"shape testing: {}, {}\".format(params['choice']['units3']))\nmodel = Sequential()\n\nmodel.add(Dense(units=round(params['units1']), input_dim = X_train.shape[1], kernel_initializer = \"glorot_uniform\")) \nmodel.add(LeakyReLU()) \nmodel.add(Dropout(params['dropout1']))\n\nmodel.add(Dense(units=round(params['units2']), kernel_initializer = \"glorot_uniform\")) \nmodel.add(LeakyReLU()) \nmodel.add(Dropout(params['dropout2']))\n\nmodel.add(Dense(units=round(params['choice']['units3']), kernel_initializer = \"glorot_uniform\")) \nmodel.add(LeakyReLU()) \nmodel.add(Dropout(params['choice']['dropout3']))    \n\nmodel.add(Dense(units=1, kernel_initializer='normal'))\nmodel.add(Activation('sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=params['optimizer'],metrics=['accuracy'])\n\nmodel.fit(X_train, y_train, epochs=round(params['nb_epochs']), batch_size=round(params['batch_size']),verbose = 0,callbacks=[my_logger], class_weight=class_weight)\n\npred_auc =model.predict_proba(X_validate, batch_size = 128, verbose = 0)\nacc = roc_auc_score(y_validate, pred_auc)\nprint('AUC:', acc)\n\nfrom sklearn.model_selection import train_test_split\nid_code_test=test[index_variable]\nX_normalized_test=normalize_data(test[numerical_variables])\ny_pred =model.predict_proba(X_normalized_test, batch_size = 128, verbose = 0)\ny_pred_rounded=np.around(y_pred)\n## submission\nsubs = pd.DataFrame(index=test.index)\nsubs['ID_code'] = test['ID_code']\nsubs['target'] = y_pred_rounded.astype(np.int64)\nsubs.to_csv('submission.csv', index=False,float_format='%.0f')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom scipy.special import logit\n\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nfeatures = [x for x in train_df.columns if x.startswith(\"var\")]\n\nhist_df = pd.DataFrame()\nfor var in features:\n    var_stats = train_df[var].append(test_df[var]).value_counts()\n    hist_df[var] = pd.Series(test_df[var]).map(var_stats)\n    hist_df[var] = hist_df[var] > 1\n\nind = hist_df.sum(axis=1) != 200\nvar_stats = {var:train_df[var].append(test_df[ind][var]).value_counts() for var in features}\n\npred = 0\nfor var in features:\n    \"\"\"\n    model = lgb.LGBMClassifier(**{ 'learning_rate': 0.04, 'num_leaves': 31, 'max_bin': 1023, 'min_child_samples': 1000, 'reg_alpha': 0.1, 'reg_lambda': 0.2,\n     'feature_fraction': 1.0, 'bagging_freq': 1, 'bagging_fraction': 0.85, 'objective': 'binary', 'n_jobs': -1, 'n_estimators':200,})\n    \"\"\"\n    \n    model = lgb.LGBMClassifier(**{ 'learning_rate':0.05, 'max_bin': 165, 'max_depth': 5, 'min_child_samples': 150,\n        'min_child_weight': 0.1, 'min_split_gain': 0.0018, 'n_estimators': 41, 'num_leaves': 6, 'reg_alpha': 2.0,\n        'reg_lambda': 2.54, 'objective': 'binary', 'n_jobs': -1})\n        \n    model = model.fit(np.hstack([train_df[var].values.reshape(-1,1),\n                                 train_df[var].map(var_stats[var]).values.reshape(-1,1)]),\n                               train_df[\"target\"].values)\n    pred += logit(model.predict_proba(np.hstack([test_df[var].values.reshape(-1,1),\n                                 test_df[var].map(var_stats[var]).values.reshape(-1,1)]))[:,1])\n    \npd.DataFrame({\"ID_code\":test_df[\"ID_code\"], \"target\":pred}).to_csv(\"submission.csv\", index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"KS - V7.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}